#!/usr/bin/env python3
'''Gscript
  Automatically generate job-scripts. The convention is used that each simulation is stored in a
  separate directory, whose name should be supplied as a command-line argument. A SLURM job-script
  will be generated in each directory (see options below for its name). The generated job-scripts
  must be submitted using "sbatch NAME" from the directory in which they are written. This can be
  done automatically using the "Gsub" command, which takes just the paths to job scripts as
  command-line arguments.

  There are three ways to define options for the ultimate job script, in order of priority:

  1. Using the command-line options. These options are applied to all paths.

  2. Using a JSON file that contains several options that should be applied to all paths. This file
     has to be specified using the "--json=<NAME>" command-line option.

  3. Using a JSON file that contains several options specific to one path. In this case path of the
     JSON file should be specified instead of the directory name. The directory name will be
     extracted from it.

Usage:
  Gscript [options] [--sbatch=<TEXT>...] [--rm=<NAME>...] <PATH>...

Arguments:
  Simulation folders, or optionally JSON files with simulation settings. The JSON files can contain
  any of the options. Options that are specified as command line arguments get priority over those
  in the JSON file.

Options:
      --json=<NAME>         Read generic settings from JSON file.
      --extension=<TEXT>    Extension of the job script. (default: .slurm)
      --filename=<TEXT>     Filename of the job script. (default: job)
      --type=<TEXT>         The type of script: tempdir, scratch, or local. (default: tempdir)
      --cmd=<TEXT>          The command(s) to execute.
      --rm=<NAME>           File to remove (before copying back to the submitdir).
  -J, --job-name=<TEXT>     Job name. (default*: path)
  -n, --nodes=<N>           Number of nodes. (default*: 1)
  -n, --ntasks=<N>          Number of tasks. (default*: 1)
  -c, --cpus-per-task=<N>   Number of CPUs-per-task. (default*: 1)
  -t, --time=<TEXT>         Maximum walltime. (default*: 1d)
  -m, --mem=<N>             Maximum memory in MB. (default*: 1024)
  -A, --account=<TEXT>      Account name.
  -p, --partition=<TEXT>    Partition name.
      --sbatch<TEXT>        Miscellaneous SBATCH options.
      --verbose             Verbose written scripts.
  -h, --help                Show help.
      --version             Show version.

* use "false" to omit

(c - MIT) T.W.J. de Geus | tom@geus.me | www.geus.me | github.com/tdegeus/GooseSLURM
'''

# ======================================== SCRIPT TEMPLATES ========================================

# ------------------------ temporary working directory on the compute node -------------------------

tempdir = '''#!/bin/bash
{sbatch:s}

# 1. Generate unique directory name [DO NOT CHANGE]
# =================================================

# get hostname
myhost=`hostname`

# get name of the temporary directory working directory, physically on the compute-node
workdir="$TMPDIR"

# get submit directory
# (every file/folder below this directory is copied to the compute node)
submitdir="${{SLURM_SUBMIT_DIR}}"

# 2. Write job info to a log file [MAY BE CHANGED/OMITTED]
# ========================================================

cat <<EOF > {filename:s}.json
{{
  "submitdir"           : "${{submitdir}}",
  "workdir"             : "${{workdir}}",
  "workdir_host"        : "${{myhost}}",
  "hostname"            : "${{myhost}}",
  "SLURM_SUBMIT_DIR"    : "${{SLURM_SUBMIT_DIR}}",
  "SLURM_JOB_ID"        : "${{SLURM_JOB_ID}}",
  "SLURM_JOB_NODELIST"  : "${{SLURM_JOB_NODELIST}}",
  "SLURM_SUBMIT_HOST"   : "${{SLURM_SUBMIT_HOST}}",
  "SLURM_JOB_NUM_NODES" : "${{SLURM_JOB_NUM_NODES}}",
  "SLURM_CPUS_PER_TASK" : "${{SLURM_CPUS_PER_TASK}}"
}}
EOF

# 3. Transfer to node [DO NOT CHANGE]
# ===================================

# create/empty the temporary directory on the compute node
if [ ! -d "${{workdir}}" ]; then
  mkdir -p "${{workdir}}"
else
  rm -rf "${{workdir}}"/*
fi

# change current directory to the location of the sbatch command
# ("submitdir" is somewhere in the home directory on the head node)
cd "${{submitdir}}"
# copy all files/folders in "submitdir" to "workdir"
# ("workdir" == temporary directory on the compute node)
cp -prf * ${{workdir}}
# change directory to the temporary directory on the compute-node
cd ${{workdir}}

# 4. Function to transfer back to the head node [DO NOT CHANGE]
# =============================================================

# define clean-up function
function clean_up {{
  # - remove local files before copying
  {remove:s}
  # - change directory to the location of the sbatch command (on the head node)
  cd "${{submitdir}}"
  # - copy everything from the temporary directory on the compute-node
  cp -prf "${{workdir}}"/* .
  # - erase the temporary directory from the compute-node
  rm -rf "${{workdir}}"/*
  rm -rf "${{workdir}}"
  # - exit the script
  exit
}}

# call "clean_up" function when this script exits, it is run even if SLURM cancels the job
trap 'clean_up' EXIT

# 5. Execute [MODIFY COMPLETELY TO YOUR NEEDS]
# ============================================

{command:s}
'''

# ---------------------------- temporary working directory on /scratch -----------------------------

scratch = '''#!/bin/bash
{sbatch:s}

# 1. Generate unique directory name [DO NOT CHANGE]
# =================================================

# get my username
username=`whoami`

# get hostname
myhost=`hostname`

# get name of the temporary directory working directory in the scratch folder
# use job id to create a unique folder
workdir="/scratch/$username/${{SLURM_JOB_ID}}"

# get submit directory
# (every file/folder below this directory is copied to the compute node)
submitdir="${{SLURM_SUBMIT_DIR}}"

# 2. Write job info to a log file [MAY BE CHANGED/OMITTED]
# ========================================================

cat <<EOF > {filename:s}.json
{{
  "submitdir"           : "${{submitdir}}",
  "workdir"             : "${{workdir}}",
  "workdir_host"        : "localhost",
  "hostname"            : "${{myhost}}",
  "SLURM_SUBMIT_DIR"    : "${{SLURM_SUBMIT_DIR}}",
  "SLURM_JOB_ID"        : "${{SLURM_JOB_ID}}",
  "SLURM_JOB_NODELIST"  : "${{SLURM_JOB_NODELIST}}",
  "SLURM_SUBMIT_HOST"   : "${{SLURM_SUBMIT_HOST}}",
  "SLURM_JOB_NUM_NODES" : "${{SLURM_JOB_NUM_NODES}}",
  "SLURM_CPUS_PER_TASK" : "${{SLURM_CPUS_PER_TASK}}"
}}
EOF

# 3. Transfer to node [DO NOT CHANGE]
# ===================================

# create/empty the temporary directory on the compute node
if [ ! -d "${{workdir}}" ]; then
  mkdir -p "${{workdir}}"
else
  rm -rf "${{workdir}}"/*
fi

# change current directory to the location of the sbatch command
# ("submitdir" is somewhere in the home directory on the head node)
cd "${{submitdir}}"
# copy all files/folders in "submitdir" to "workdir"
# ("workdir" == temporary directory on the compute node)
cp -prf * ${{workdir}}
# change directory to the temporary directory on the compute-node
cd ${{workdir}}

# 4. Function to transfer back to the head node [DO NOT CHANGE]
# =============================================================

# define clean-up function
function clean_up {{
  # - remove local files before copying
  {remove:s}
  # - change directory to the location of the sbatch command (on the head node)
  cd "${{submitdir}}"
  # - copy everything from the temporary directory on the compute-node
  cp -prf "${{workdir}}"/* .
  # - erase the temporary directory from the compute-node
  rm -rf "${{workdir}}"/*
  rm -rf "${{workdir}}"
  # - exit the script
  exit
}}

# call "clean_up" function when this script exits, it is run even if SLURM cancels the job
trap 'clean_up' EXIT

# 5. Execute [MODIFY COMPLETELY TO YOUR NEEDS]
# ============================================

{command:s}
'''

# -------------------------------------- read/write from home --------------------------------------

local = '''#!/bin/bash
{sbatch:s}

# 1. Write job info to a log file [MAY BE CHANGED/OMITTED]
# ========================================================

# get hostname
myhost=`hostname`

# get the working directory as the current directory
workdir=`pwd`

# get submit directory
submitdir="${{SLURM_SUBMIT_DIR}}"

cat <<EOF > {filename:s}.json
{{
  "submitdir"           : "${{submitdir}}",
  "workdir"             : "${{workdir}}",
  "workdir_host"        : "localhost",
  "hostname"            : "${{myhost}}",
  "SLURM_SUBMIT_DIR"    : "${{SLURM_SUBMIT_DIR}}",
  "SLURM_JOB_ID"        : "${{SLURM_JOB_ID}}",
  "SLURM_JOB_NODELIST"  : "${{SLURM_JOB_NODELIST}}",
  "SLURM_SUBMIT_HOST"   : "${{SLURM_SUBMIT_HOST}}",
  "SLURM_JOB_NUM_NODES" : "${{SLURM_JOB_NUM_NODES}}",
  "SLURM_CPUS_PER_TASK" : "${{SLURM_CPUS_PER_TASK}}"
}}
EOF

# 2. Execute [MODIFY COMPLETELY TO YOUR NEEDS]
# ============================================

{command:s}

{remove:s}
'''

# ========================================= LOAD LIBRARIES =========================================

import os, sys, re, docopt, json

# ========================================== MAIN PROGRAM ==========================================

# ---------------------------------- parse command line arguments ----------------------------------

# parse command-line options
args = docopt.docopt(__doc__,version='0.0.4')

# change keys to simplify implementation:
# - remove leading "-" and "--" from options
args = {re.sub(r'([\-]{1,2})(.*)',r'\2',key): args[key] for key in args}
# - remove "<...>"
args = {re.sub(r'(<)(.*)(>)',r'\2',key): args[key] for key in args}

# ---------------------------------------- modify arguments ----------------------------------------

# remove empty options
for key in [key for key in args]:
  if args[key] is None:
    del args[key]
  elif type(args[key])==list:
    if len(args[key])==0:
      del args[key]

# read options from JSON file
if args['json']:
  # - read file
  info = json.load(open(args['json'],'r'))
  # - overwrite options
  for key, text in info.items():
    if key not in args:
      args[key] = text

# ------------------------------------------ write files -------------------------------------------

# loop over arguments
for dirname in args.pop('PATH'):

  # initialize options
  info = {}

  # read options
  if os.path.isfile(dirname):
    # - read file
    info = json.load(open(dirname,'r'))
    # - extract directory name
    dirname, fname = os.path.split(dirname)

  # overwrite options
  for key, text in args.items():
    info[key] = text

  # set default options
  info.setdefault('extension'     , '.slurm'  )
  info.setdefault('filename'      , 'job'     )
  info.setdefault('type'          , 'tempdir' )
  info.setdefault('job-name'      , dirname   )
  info.setdefault('nodes'         , '1'       )
  info.setdefault('ntasks'        , '1'       )
  info.setdefault('cpus-per-task' , '1'       )
  info.setdefault('time'          , '1d'      )
  info.setdefault('mem'           , '1024'    )

  # check to ignore options
  for key in ['time','mem','cpus-per-task','job-name','nodes','ntasks']:
    if info[key].lower() in ['none', 'false']:
      del info[key]

  # convert time
  if 'time' in info:
    if not re.match('[0-9]*\:[0-9]*\:[0-9]*',info['time']):
      # - alias
      T = info['time']
      # - convert input to seconds
      if   T[-1] == 'd': T = float(T[:-1]) * float(60*60*24)
      elif T[-1] == 'h': T = float(T[:-1]) * float(60*60)
      elif T[-1] == 'm': T = float(T[:-1]) * float(60)
      elif T[-1] == 's': T = float(T[:-1]) * float(1)
      else             : T = float(T)
      # - convert seconds back to hours:minutes:seconds
      T = int(T)
      s = int( T % 60 );  T = ( T - s ) / 60
      m = int( T % 60 );  T = ( T - m ) / 60
      h = int( T )
      T = '%d:%02d:%02d' % (h,m,s)
      # - alias
      info['time'] = T

  # set sbatch options
  # - initialize
  sbatch = []
  # - add supported sbatch options
  if 'job-name'      in info: sbatch += ['#SBATCH --job-name {}'     .format(info['job-name'     ])]
  if 'filename'      in info: sbatch += ['#SBATCH --out {}.out'      .format(info['filename'     ])]
  if 'nodes'         in info: sbatch += ['#SBATCH --nodes {}'        .format(info['nodes'        ])]
  if 'ntasks'        in info: sbatch += ['#SBATCH --ntasks {}'       .format(info['ntasks'       ])]
  if 'cpus-per-task' in info: sbatch += ['#SBATCH --cpus-per-task {}'.format(info['cpus-per-task'])]
  if 'mem'           in info: sbatch += ['#SBATCH --mem {}'          .format(info['mem'          ])]
  if 'time'          in info: sbatch += ['#SBATCH --time {}'         .format(info['time'         ])]
  if 'partition'     in info: sbatch += ['#SBATCH --partition {}'    .format(info['partition'    ])]
  if 'account'       in info: sbatch += ['#SBATCH --account {}'      .format(info['account'      ])]
  # - add arbitrary other sbatch options
  if 'sbatch' in info:
    for opt in info['sbatch']:
      sbatch += ['#SBATCH {}'.format(opt)]
  # - convert to string
  sbatch = '\n'.join(sbatch)

  # convert command
  # - initialize
  command = ''
  # - read
  if 'cmd' in info:
    if type(info['cmd']) == list: command = '\n'.join(info['cmd'])
    else                        : command =           info['cmd']

  # convert remove files
  # - initialize
  remove = ''
  # - read
  if 'rm' in info:
    if type(info['rm']) == list: remove = 'rm -r '+' '.join(info['rm'])
    else                       : remove = 'rm -r '+         info['rm']

  # convert filename
  filename = info['filename'] + info['extension']

  # get job script
  if   info['type'] == 'tempdir': text = tempdir
  elif info['type'] == 'scratch': text = scratch
  elif info['type'] == 'local'  : text = local

  # create directory if it does not exist
  if not os.path.isdir(dirname): os.makedirs(dirname)

  # write file
  open(os.path.join(dirname,filename),'w').write(text.format(
    sbatch   = sbatch,
    filename = filename,
    command  = command,
    remove   = remove,
  ))

  # print progress
  if info['verbose']: print('Written "{}"'.format(os.path.join(dirname,filename)))
